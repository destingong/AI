{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/destingong/AI/blob/master/AIOps_notebook_20231104.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN1SPFLFQMbV"
      },
      "source": [
        "## **LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D6F8ykguOLM",
        "outputId": "704383a3-ba7b-45ee-ef0b-a70b0fc85aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.350)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.2)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.0)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.69)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BB-iqrCuMTXs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hf_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyFD82JTRKm7"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "veZ29gMnM5kE"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbdsBKD2QAVG",
        "outputId": "67f0ad60-077e-4750-aace-2434c621af9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from langchain import HuggingFaceHub\n",
        "llm = HuggingFaceHub(repo_id = \"google/flan-t5-xxl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo-Qr4uG2D0O"
      },
      "outputs": [],
      "source": [
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIgGM1AzNHFw"
      },
      "source": [
        "### Solution Architect QA Agent\n",
        "- input: user prompt\n",
        "- output: string of AWS architect\n",
        "- context\n",
        "- instruction\n",
        "\n",
        "#### Prompt Engineering\n",
        "- PromptTemplate and ChatPromptTemplate implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\n",
        "\n",
        "- PromptTemplate accepts a dictionary (of the prompt variables) and returns a StringPromptValue. A ChatPromptTemplate accepts a dictionary and returns a ChatPromptValue."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # [template] ChatPromptTemplate - SystemMessage, HumanMessagePromptTemplate\n",
        "# from langchain.prompts import ChatPromptTemplate\n",
        "# from langchain.prompts import HumanMessagePromptTemplate\n",
        "# from langchain.schema.messages import SystemMessage\n",
        "\n",
        "# chat_template = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         SystemMessage(\n",
        "#             content=(\n",
        "#                 \"You are a helpful assistant that re-writes the user's text to \"\n",
        "#                 \"sound more upbeat.\"\n",
        "#             )\n",
        "#         ),\n",
        "#         HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# prompt = chat_template.invoke({'text': \"i dont like eating tasty things.\"}).to_string()\n",
        "# prompt"
      ],
      "metadata": {
        "id": "CbVCPnCm5ZpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdT9CUO1LL44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Template 1] Simplest PromptTemplate with User Input"
      ],
      "metadata": {
        "id": "m3W-_ZpcLMki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rHF-zR1NNvI"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "question = \"What is a Machine Learning web app architecture in AWS?\"\n",
        "\n",
        "template = \"\"\"\n",
        "    Human: {question}\n",
        "    You are helpful system architect that designs AWS architecture based on user's question, based on the aspects of Compute, Network, Storage.\n",
        "    In details.\n",
        "    Assistant:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=['question'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Template 2] Few Shot Prompt Template"
      ],
      "metadata": {
        "id": "E4l6wwfdClBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = [\n",
        "    {\"question\": \"Design a QnA web app architecture in AWS?\",\n",
        "     \"answer\": \"\"\"\n",
        "        Step 1\n",
        "        Deploy this AWS Solution into your AWS account. Open the Content Designer user interface (UI) or the Amazon Lex web client, and use Amazon Cognito to authenticate.\n",
        "\n",
        "\n",
        "\n",
        "        Step 2\n",
        "        After authentication, Amazon API Gateway and Amazon Simple Storage Service (Amazon S3) deliver the contents of the Content Designer UI.\n",
        "\n",
        "\n",
        "\n",
        "        Step 3\n",
        "        Configure questions and answers in the Content Designer. The UI sends requests to Amazon API Gateway to save the questions and answers.\n",
        "\n",
        "\n",
        "\n",
        "        Step 4\n",
        "        The Content Designer AWS Lambda function saves the input in Amazon OpenSearch Service in a question's bank index. If using text embeddings, these request pass through a machine learning (ML) model, hosted on Amazon SageMaker, to generate embeddings before being saved into the question bank on OpenSearch.\n",
        "\n",
        "        Step 5\n",
        "        Chatbot users interact with Amazon Lex through the web client UI or Amazon Connect.\n",
        "\n",
        "\n",
        "\n",
        "        Step 6\n",
        "        Amazon Lex forwards requests to the Bot Fulfillment Lambda function. Chatbot users can also send requests to this Lambda function through Amazon Alexa devices.\n",
        "\n",
        "\n",
        "\n",
        "        Step 7\n",
        "        The Bot Fulfillment Lambda function takes the user's input and uses Amazon Comprehend and Amazon Translate (if necessary) to translate non-English requests to English, and then looks up the answer in OpenSearch. If using large language model (LLM) features, such as text generation and text embeddings, these requests first pass through various ML models hosted on SageMaker. SageMaker generates the search query and embeddings to compare with those saved in the question bank on OpenSearch.\n",
        "\n",
        "\n",
        "\n",
        "        Step 8\n",
        "        If an Amazon Kendra index is configured for fallback, the Bot Fulfillment Lambda function forwards the request to Amazon Kendra if no matches were returned from the OpenSearch question bank. The text generation LLM can be used to create the search query, and to synthesize a response from the excerpts of the returned document.\n",
        "\n",
        "\n",
        "     \"\"\"}\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
        "print(example_prompt.format(**example[0]))"
      ],
      "metadata": {
        "id": "0rZgc8d2BcgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=example,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Question: {question}\",\n",
        "    input_variables=[\"question\"]\n",
        ")\n",
        "\n",
        "#print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
      ],
      "metadata": {
        "id": "4XVH2A9kEfri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain = prompt | llm\n",
        "llm_output = chain.invoke({\"question\": question})\n",
        "llm_output"
      ],
      "metadata": {
        "id": "l-P8dqgrILSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Template 3] Transformation Text to JSON"
      ],
      "metadata": {
        "id": "gBY6kkkgBDs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "    Extract all input and output AWS services mentioned in each Step from the text delimitted by three backticks below.\n",
        "\n",
        "    Format your response as a list of JSON objects where each object has the following keys: \\\n",
        "    \"input\", \"output\" for each Step in text.\n",
        "\n",
        "\n",
        "    As an example: \\\n",
        "    ['{{input: Content Designer user interface (UI), output: Amazon Cognito}}', \\\n",
        "     '{{input: Amazon S3, output: Content Designer user interface (UI)}}', \\\n",
        "     ...]\n",
        "\n",
        "    Please return unique values.\\\n",
        "\n",
        "    Text:\n",
        "    '''{text}'''\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tD6x4wtrLwa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# template = \"\"\"\n",
        "#     Extract all nouns and verbs mentioned in each Step from the text delimitted by three backticks below.\n",
        "\n",
        "#     Format your response as a list of JSON objects where each object has the following keys: \\\n",
        "#     \"noun\", \"verb\" for each Step in text.\n",
        "\n",
        "#     Please return unique values.\n",
        "\n",
        "#     As an example: \\\n",
        "\n",
        "#     ['{{noun: Amazon Cognito, verb: authenticate}}', \\\n",
        "#      '{{noun: Content Designer user interface (UI), verb: open}}']\n",
        "\n",
        "#     Text:\n",
        "#     '''{text}'''\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "CyMxGEA-mUD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "\n",
        "repo_list = [\"lmsys/fastchat-t5-3b-v1.0\",  # return one row only\n",
        "             \"databricks/dolly-v2-7b\",  # timeout\n",
        "             \"google/flan-t5-base\",\n",
        "             \"gpt2\"] # return '\\n'\n",
        "\n",
        "for repo_id in repo_list:\n",
        "    textToJson_model = HuggingFaceHub(repo_id = repo_id)"
      ],
      "metadata": {
        "id": "rasQA5KNBCxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#textToJson_model = HuggingFaceHub(repo_id = 'databricks/dolly-v2-3b')\n",
        "textToJson_model = HuggingFaceHub(repo_id = 'google/flan-t5-xxl')"
      ],
      "metadata": {
        "id": "zxczJ9nQVsm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output"
      ],
      "metadata": {
        "id": "bH2FO2ijR6qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "keyword_extract_prompt = PromptTemplate(template=template, input_variables=['text'])\n",
        "chain = keyword_extract_prompt | textToJson_model\n",
        "keyword_output = chain.invoke({\"text\": llm_output})\n",
        "keyword_output"
      ],
      "metadata": {
        "id": "IQCXwmqdiptZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_list = keyword_output[3:-2].split(\"', '\")\n",
        "#keyword_dict = {[i.split(\",\")[0].replace(\"noun: \", \"\") for i in keyword_list]:[i.split(\",\")[1].replace(\"verb: \", \"\") for i in keyword_list] }\n",
        "keyword_list"
      ],
      "metadata": {
        "id": "MBYVfLl9ROKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "keyword_list = output_parser.parse(keyword_output)"
      ],
      "metadata": {
        "id": "mUjSkiCeNa8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network Graph\n",
        "- https://towardsdatascience.com/customizing-networkx-graphs-f80b4e69bedf"
      ],
      "metadata": {
        "id": "po4lL06IWkcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "G = nx.Graph()\n",
        "\n",
        "for i in keyword_list:\n",
        "    print(i)\n",
        "    input = i.split(\",\")[0].replace(\"input: \", \"\")\n",
        "    output = i.split(\",\")[1].replace(\"output: \", \"\")\n",
        "\n",
        "    G.add_node(input)\n",
        "    G.add_node(output)\n",
        "    G.add_edge(input, output, weight=1.7)\n",
        "\n",
        "nx.draw(G, with_labels = True, node_size=100)"
      ],
      "metadata": {
        "id": "7dwo7HINK54Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Template 4] Prompt with Different Roles\n",
        "- Human message, AI message, Assistant message\n",
        "- MessagePlaceholder: LangChain also provides MessagesPlaceholder, which gives you full control of what messages to be rendered during formatting. This can be useful when you are uncertain of what role you should be using for your message prompt templates or when you wish to insert a list of messages during formatting.\n"
      ],
      "metadata": {
        "id": "5Ve6a7APXWg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import MessagesPlaceholder\n",
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "human_prompt = \"Show me the transformed {text}\"\n",
        "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([MessagesPlaceholder(variable_name=\"text\"), human_message_template])\n",
        "\n",
        "\n",
        "from langchain.schema.messages import HumanMessage, AIMessage\n",
        "\n",
        "human_message = HumanMessage(content=\"Show me the transformed output\")\n",
        "\n",
        "ai_message = AIMessage(content= \"\"\"\n",
        "    Extract all nouns and verbs mentioned in each Step from the text delimitted by three backticks below.\n",
        "\n",
        "    Format your response as a list of JSON objects where each object has the following keys: \\\n",
        "    \"noun\", \"verb\" for each Step in text.\n",
        "\n",
        "    Please return unique values.\n",
        "\n",
        "    As an example: \\\n",
        "\n",
        "    ['{{noun: Amazon Cognito, verb: authenticate}}', \\\n",
        "     '{{noun: Content Designer user interface (UI), verb: open}}']\n",
        "\n",
        "\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "chat_prompt.format_prompt(conversation=[human_message, ai_message], text = llm_output).to_messages()\n",
        "\n",
        "chat_prompt"
      ],
      "metadata": {
        "id": "V9TEk87nXUb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SYS_PROMPT = \"\"\"\n",
        "#     \"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
        "#     \"You are provided with a context chunk {{model_output}} Your task is to extract the ontology \"\n",
        "#     \"of terms mentioned in the given context. These terms should represent the key concepts as per the context. \\n\"\n",
        "#     \"Thought 1: While traversing through each sentence, Think about the key terms mentioned in it.\\n\"\n",
        "#         \"\\tTerms may include object, entity, location, organization, person, \\n\"\n",
        "#         \"\\tcondition, acronym, documents, service, concept, etc.\\n\"\n",
        "#         \"\\tTerms should be as atomistic as possible\\n\\n\"\n",
        "#     \"Thought 2: Think about how these terms can have one on one relation with other terms.\\n\"\n",
        "#         \"\\tTerms that are mentioned in the same sentence or the same paragraph are typically related to each other.\\n\"\n",
        "#         \"\\tTerms can be related to many other terms\\n\\n\"\n",
        "#     \"Thought 3: Find out the relation between each such related pair of terms. \\n\\n\"\n",
        "#     \"Format your output as a list of json. Each element of the list contains a pair of terms\"\n",
        "#     \"and the relation between them, like the follwing: \\n\"\n",
        "#     \"[\\n\"\n",
        "#     \"   {\\n\"\n",
        "#     '       \"node_1\": \"A concept from extracted ontology\",\\n'\n",
        "#     '       \"node_2\": \"A related concept from extracted ontology\",\\n'\n",
        "#     '       \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\\n'\n",
        "#     \"   }, {...}\\n\"\n",
        "#     \"]\"\n",
        "# \"\"\"\n",
        "\n",
        "# USER_PROMPT = \"context: ```{input}``` \\n\\n output: \""
      ],
      "metadata": {
        "id": "0ajSsxv0g1W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# doc = nlp(output)\n",
        "\n",
        "# token_dict = {'node':[], 'relationship':[]}\n",
        "\n",
        "# for index, token in enumerate(doc):\n",
        "#     if token.pos_ == 'PROPN' or token.pos_ == 'NOUN':\n",
        "#         token_dict['node'].append(token)\n",
        "#     if token.pos_ == 'VERB':\n",
        "#         token_dict['relationship'] = token\n",
        "# token_dict"
      ],
      "metadata": {
        "id": "VCf8mvqJLIU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQyBfS15AeJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # output parser data structure\n",
        "# from langchain.output_parsers import PydanticOutputParser\n",
        "# from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "\n",
        "# class Graph(BaseModel):\n",
        "#     node: str = Field(description='entity in a sentence')\n",
        "#     relationship: str = Field(description='verb in a sentence')\n",
        "\n",
        "# parser = PydanticOutputParser(pydantic_object=Graph)"
      ],
      "metadata": {
        "id": "P9YjNaa_FQtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z97r9-VIPsJ4"
      },
      "source": [
        "### Extract Knowledge Graph from Text Agent\n",
        "- input: string of AWS architects\n",
        "- output: knowledge graph components node (source, destination), edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDbevIvgWthr"
      },
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRpV6tJ-PphU"
      },
      "outputs": [],
      "source": [
        "# #from langchain.document_loaders import WikipediaLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "# from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "\n",
        "# # Read the wikipedia article\n",
        "# raw_documents = 'how are you'\n",
        "# # Define chunking strategy\n",
        "# text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "#     chunk_size=3, chunk_overlap=0\n",
        "# )\n",
        "\n",
        "# # Only take the first the raw_documents\n",
        "# documents = text_splitter.split_text(raw_documents)\n",
        "# documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## spacy\n",
        "# https://spacy.io/models"
      ],
      "metadata": {
        "id": "FpRLXWd14iWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAk1osfISWvI"
      },
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyIvreS6Sf5A"
      },
      "outputs": [],
      "source": [
        "# pip install pyvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3KOpgFESpHd"
      },
      "outputs": [],
      "source": [
        "# from pyvis import network as net\n",
        "# net = net.Network(height='400px', width='50%', notebook=True)\n",
        "\n",
        "# net.add_node(1)\n",
        "# net.add_node(2)\n",
        "# net.add_edge(1, 2)\n",
        "\n",
        "# net.save_graph(\"graph.html\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0CVvCkzT9xZ"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import HTML\n",
        "\n",
        "display(net.show(\"graph.html\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgEcQnft78hJ"
      },
      "source": [
        "## SQL Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYqHxuKglfTw"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeacpklLuIDe"
      },
      "outputs": [],
      "source": [
        "from langchain.utilities import SQLDatabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVc0WrS0ucII"
      },
      "outputs": [],
      "source": [
        "db = SQLDatabase.from_uri(\"sqlite:///./Chinook.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tZe0de2ueDS"
      },
      "outputs": [],
      "source": [
        "def get_schema(_):\n",
        "    return db.get_table_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N68j0IvwugNF"
      },
      "outputs": [],
      "source": [
        "def run_query(query):\n",
        "    return db.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv0S_V0buhsG"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "sql_response = (\n",
        "    RunnablePassthrough.assign(schema=get_schema)\n",
        "    | prompt\n",
        "    | llm.bind(stop=[\"\\nSQLResult:\"])\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otx_fF3yR4vV"
      },
      "outputs": [],
      "source": [
        "sql_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx9pfzOpujAv"
      },
      "outputs": [],
      "source": [
        "sql_response.invoke({\"question\": \"How many employees are there?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIwcHh65uj7k"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
        "{schema}\n",
        "\n",
        "Question: {question}\n",
        "SQL Query: {query}\n",
        "SQL Response: {response}\"\"\"\n",
        "prompt_response = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgVflt0Huo6D"
      },
      "outputs": [],
      "source": [
        "full_chain = (\n",
        "    RunnablePassthrough.assign(query=sql_response)\n",
        "    | RunnablePassthrough.assign(\n",
        "        schema=get_schema,\n",
        "        response=lambda x: db.run(x[\"query\"]),\n",
        "    )\n",
        "    | prompt_response\n",
        "    | llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkRXs2nUuqhG"
      },
      "outputs": [],
      "source": [
        "full_chain.invoke({\"question\": \"How many employees are there?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZcoNodcusQz"
      },
      "outputs": [],
      "source": [
        "    AIMessage(content='There are 8 employees.', additional_kwargs={}, example=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "m3W-_ZpcLMki"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPA5t7qEt8y3kSejh+Q+Jrg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}